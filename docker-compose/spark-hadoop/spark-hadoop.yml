# Simple Spark and Hadoop setup for testing.

version: '2'
services:
  namenode:
    image: analytics/hadoop-namenode
    hostname: namenode
    container_name: namenode
    volumes:
      - ./data/namenode:/hadoop/dfs/name
      - ./data/exchange:/exchange
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ../hadoop.env
    ports:
      - "50070:50070"
      - "8020:8020"

  datanode1:
    image: analytics/hadoop-datanode
    hostname: datanode1
    volumes:
      - ../data/datanode1:/hadoop/dfs/data
    env_file:
      - ../hadoop.env
    depends_on:
      - namenode

  spark-master:
    image: analytics/spark-master
    depends_on:
      - namenode
      - datanode1
    container_name: spark-master
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - SPARK_CONF_spark_eventLog_enabled=true
      - SPARK_CONF_spark_eventLog_dir=hdfs://namenode:8020/spark-logs
      - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode:8020/spark-logs    
    env_file:
      - ../hadoop.env
    ports:
      - "8080:8080"
    volumes:
      - ~/datascience-cluster/spark:/spark-scratch

  spark-worker:
    image: analytics/spark-worker
    depends_on:
        - spark-master
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - SPARK_CONF_spark_eventLog_enabled=true
      - SPARK_CONF_spark_eventLog_dir=hdfs://namenode:8020/spark-logs
      - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode:8020/spark-logs
      - SPARK_MASTER_URL=spark://spark-master:7077
    env_file:
      - ../hadoop.env
    volumes_from:
      - spark-master
